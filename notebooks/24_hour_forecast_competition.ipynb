{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import pickle\n",
    "import tables\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from comet_ml.api import API, APIExperiment\n",
    "from comet_ml.query import Tag\n",
    "\n",
    "if not '..' in sys.path:\n",
    "    sys.path.append('..')\n",
    "from train_LSTM import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, convert all the Excel spreadsheets to pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_holidays = {\n",
    "    2019: [\n",
    "        [1, 1],\n",
    "        [4, 19],\n",
    "        [4, 21],\n",
    "        [4, 25],\n",
    "        [5, 1],\n",
    "        [6, 10],\n",
    "        [6, 20],\n",
    "        [8, 15],\n",
    "        [10, 5],\n",
    "        [11, 1],\n",
    "        [12, 1],\n",
    "        [12, 8],\n",
    "        [12, 25]\n",
    "    ],\n",
    "    2020: [\n",
    "        [1, 1],\n",
    "        [4, 10],\n",
    "        [4, 12],\n",
    "        [4, 25],\n",
    "        [5, 1],\n",
    "        [6, 10],\n",
    "        [6, 11],\n",
    "        [8, 15],\n",
    "        [10, 5],\n",
    "        [11, 1],\n",
    "        [12, 1],\n",
    "        [12, 8],\n",
    "        [12, 25]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sheet(data_file, sheet_name, header=(0,)):\n",
    "    df = pd.read_excel(data_file, sheet_name=sheet_name, parse_dates=True, header=header)\n",
    "    \n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        cols = []\n",
    "        for col in df.columns.to_list():\n",
    "            if 'Unnamed' in col[0]:\n",
    "                cols.append(col[1])\n",
    "            else:\n",
    "                cols.append(col[0].split(' ')[-1] + '_' + col[1].split(' ')[0])\n",
    "        df.columns = cols\n",
    "\n",
    "    if 'date' in df.columns:\n",
    "        year = df.date[0].year\n",
    "        df['datetime'] = [pd.Timestamp(year=d.year, month=d.month, day=d.day, \n",
    "                                       hour=t.hour, minute=t.minute) for d,t in zip(df.date, df.time)]\n",
    "        df.drop(['date', 'time'], axis=1, inplace=True)\n",
    "    elif 'datetime (epoch)' in df.columns:\n",
    "        datetimes = [time.gmtime(dt) for dt in df['datetime (epoch)']]\n",
    "        year = datetimes[0].tm_year\n",
    "        df['datetime'] = [pd.Timestamp(year = dt.tm_year, month = dt.tm_mon, day = dt.tm_mday, \n",
    "                                 hour = dt.tm_hour, minute  = dt.tm_min) for dt in datetimes]\n",
    "        df.drop('datetime (epoch)', axis=1, inplace=True)\n",
    "\n",
    "    df['weekday'] = [d.weekday() for d in df['datetime']]\n",
    "    df['weekend'] = [d.weekday() in (5,6) for d in df['datetime']]\n",
    "    df['holiday'] = False\n",
    "    for holiday in bank_holidays[year]:\n",
    "        idx = np.array([timestamp.month == holiday[0] and timestamp.day == holiday[1] \\\n",
    "                        for timestamp in df['datetime']])\n",
    "        df.loc[idx, 'holiday'] = True\n",
    "\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-4:] + cols[:-4]\n",
    "    df = df[cols]\n",
    "    df.rename({col: col.split(' ')[0] for col in cols}, axis='columns', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/'\n",
    "xls_files = ['data1.xlsx','data2.xlsx','data3.1_mon_10.xlsx',\n",
    "             'data3.2_tue_11.xlsx','data3.3_wed_12-1.xlsx',\n",
    "             'data3.4_thu_13.xlsx','data3.5_fri_14.xlsx']\n",
    "pkl_files = [os.path.splitext(xls_file)[0] + '.pkl' for xls_file in xls_files]\n",
    "reload = False\n",
    "data = {}\n",
    "for xls_file, pkl_file in zip(xls_files, pkl_files):\n",
    "    if not os.path.isfile(data_folder + pkl_file) or reload:\n",
    "        print(f'Processing {data_folder}{pkl_file}')\n",
    "        building_energy = read_sheet(data_folder + xls_file, 'building_energy')\n",
    "        building_sensor = read_sheet(data_folder + xls_file, 'building_sensor')\n",
    "        weather_data = read_sheet(data_folder + xls_file, 'weather_data')\n",
    "        zones = {i+1: read_sheet(data_folder + xls_file, sheet_name=f'zone#{i+1}_energy', \n",
    "                                 header=(0,1)) for i in range(5)}\n",
    "        sensors = {i+1: read_sheet(data_folder + xls_file, sheet_name=f'zone#{i+1}_sensor') for i in range(3)}\n",
    "        data = {'full': {\n",
    "            'building_energy': building_energy,\n",
    "            'building_sensor': building_sensor,\n",
    "            'weather_data': weather_data,\n",
    "            'zones': zones,\n",
    "            'sensors': sensors\n",
    "        }}\n",
    "        pickle.dump(data, open(data_folder + pkl_file, 'wb'))\n",
    "    blob = pickle.load(open(data_folder + pkl_file, 'rb'))\n",
    "    for key1 in blob:\n",
    "        if key1 not in data:\n",
    "            data[key1] = blob[key1]\n",
    "        else:\n",
    "            for key2,value2 in blob[key1].items():\n",
    "                if isinstance(value2, dict):\n",
    "                    for key3,value3 in blob[key1][key2].items():\n",
    "                        data[key1][key2][key3] = data[key1][key2][key3].append(value3, ignore_index=True)\n",
    "                elif isinstance(value2, pd.DataFrame):\n",
    "                    data[key1][key2] = data[key1][key2].append(value2, ignore_index=True)\n",
    "                else:\n",
    "                    raise Exception(f'Do not know how to deal with object of type {type(value2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the best models\n",
    "\n",
    "The various combination of hyperparameters tested lead to the following values of validation loss:\n",
    "\n",
    "| Consumption | Generation | Temperature | Future size | Validation loss |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| Y | N | N | 1 | 0.1457 &pm; 0.0044 |\n",
    "| Y | Y | N | 1 | 0.1527 &pm; 0.0044 |\n",
    "| Y | Y | Y | 1 | 0.1524 &pm; 0.0042 |\n",
    "| Y | N | N | 0.5 | 0.1460 &pm; 0.0033 |\n",
    "| Y | Y | N | 0.5 | 0.1448 &pm; 0.0033 |\n",
    "| Y | Y | Y | 0.5 | 0.1477 &pm; 0.0032 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = API(api_key = os.environ['COMET_API_KEY'])\n",
    "workspace = 'danielelinaro'\n",
    "project_name = 'load-forecasting'\n",
    "\n",
    "future = 0.5  # [hours]\n",
    "history = 24  # [hours]\n",
    "hours_ahead = np.r_[0 : 24 : future]\n",
    "\n",
    "n_neurons = 20\n",
    "n_layers = 3\n",
    "with_building_temperature = False\n",
    "consumption_only = False\n",
    "random_initial_weights = True\n",
    "training_data_files = ['data1','data2']\n",
    "\n",
    "base_query = Tag('LSTM') & Tag('_'.join(training_data_files)) & \\\n",
    "    Tag(f'future={future:.2f}') & Tag(f'history={history}')\n",
    "if n_layers is not None:\n",
    "    base_query &= Tag(f'{n_layers}_layers')\n",
    "if n_neurons is not None:\n",
    "    base_query &= Tag(f'{n_neurons}_neurons')\n",
    "if with_building_temperature:\n",
    "    base_query &= Tag('building_temperature')\n",
    "if consumption_only:\n",
    "    base_query  &= Tag('consumption_only')\n",
    "if random_initial_weights:\n",
    "    base_query &= Tag('random_initial_weights')\n",
    "else:\n",
    "    base_query &= Tag('initialized_weights')\n",
    "\n",
    "experiments_path = '../experiments/LSTM/'\n",
    "experiment_IDs = []\n",
    "val_losses = []\n",
    "for hours in hours_ahead:\n",
    "    msg = f'{hours:5.2f} hours ahead '\n",
    "    n_chars = len(msg)\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "    query = base_query & Tag(f'ahead={hours:.1f}')\n",
    "    experiments = api.query(workspace, project_name, query, archived=False)\n",
    "    if not with_building_temperature:\n",
    "        experiments = [expt for expt in experiments \\\n",
    "                       if all([tag != 'building_temperature' for tag in expt.get_tags()])]\n",
    "    if not consumption_only:\n",
    "        experiments = [expt for expt in experiments \\\n",
    "                       if all([tag != 'consumption_only' for tag in expt.get_tags()])]\n",
    "    msg = f'({len(experiments)} expts): '\n",
    "    n_chars += len(msg)\n",
    "    sys.stdout.write(msg)\n",
    "    min_val_loss = 100\n",
    "    for experiment in experiments:\n",
    "        metrics = experiment.get_metrics_summary()\n",
    "        loss = [float(m['valueMin']) for m in metrics if m['name'] == 'val_loss'][0]\n",
    "        if loss < min_val_loss:\n",
    "            ID = experiment.id\n",
    "            history = pickle.load(open(experiments_path + ID + '/history.pkl','rb'))\n",
    "            val_loss = history['val_loss']\n",
    "            min_val_loss = loss\n",
    "            #tags = all_tags[experiments.index(experiment)]\n",
    "            tags = experiment.get_tags()\n",
    "    experiment_IDs.append(ID)\n",
    "    val_losses.append(val_loss)\n",
    "    n_neurons = [int(tag.split('_')[0]) for tag in tags if 'neurons' in tag][0]\n",
    "    n_layers = [int(tag.split('_')[0]) for tag in tags if 'layers' in tag][0]\n",
    "    n_epochs = len(val_loss)\n",
    "    msg = f'best expt is {ID[:9]} ({n_layers} layers, {n_neurons} neurons, {n_epochs:3d} epochs), ' + \\\n",
    "        f'validation loss: {min_val_loss:.4f}'\n",
    "    print(msg)\n",
    "    n_chars += len(msg)\n",
    "\n",
    "print('=' * n_chars)\n",
    "min_val_losses = np.array([np.min(val_loss) for val_loss in val_losses])\n",
    "m = min_val_losses.mean()\n",
    "s = min_val_losses.std()\n",
    "sem = s / np.sqrt(min_val_losses.size)\n",
    "print(f'Validation loss: {m:.4f} +- {sem:.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_experiments = len(experiment_IDs)\n",
    "models = []\n",
    "for i,experiment_ID in enumerate(experiment_IDs):\n",
    "    checkpoint_path = experiments_path + experiment_ID + '/checkpoints/'\n",
    "    checkpoint_files = glob.glob(checkpoint_path + '*.h5')\n",
    "    epochs = [int(os.path.split(file)[-1].split('.')[1].split('-')[0])\n",
    "              for file in checkpoint_files]\n",
    "    best_checkpoint = checkpoint_files[epochs.index(np.argmin(val_losses[i]) + 1)]\n",
    "    models.append(keras.models.load_model(best_checkpoint, compile=True))\n",
    "\n",
    "parameters = pickle.load(open(experiments_path + experiment_ID + '/parameters.pkl', 'rb'))\n",
    "# we need min and max of the training set to normalize the data\n",
    "training_set_min = parameters['training_set_min']\n",
    "training_set_max = parameters['training_set_max']\n",
    "try:\n",
    "    data_files = ['../' + data_file for data_file in parameters['data_files']]\n",
    "except:\n",
    "    data_files = ['../' + parameters['data_file']]\n",
    "time_step = parameters['time_step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = ['../data/data2.pkl', '../data/data3.1_mon_10.pkl',\n",
    "               '../data/data3.2_tue_11.pkl', '../data/data3.3_wed_12-1.pkl',\n",
    "               '../data/data3.4_thu_13.pkl', '../data/data3.5_fri_14.pkl']\n",
    "input_file = input_files[5]\n",
    "data_full = pickle.load(open(input_file, 'rb'))['full']\n",
    "data = data_full['building_energy'].copy()\n",
    "if 'building_consumption' in parameters['inputs']['continuous']:\n",
    "    data.rename({key: 'building_' + key for key in ('consumption','generation')},\n",
    "                axis='columns', inplace=True)\n",
    "data['building_temperature'] = data_full['building_sensor']['temperature'].copy()\n",
    "orig_time_step = extract_time_step(data)\n",
    "data = add_minute_and_workday(data)\n",
    "data = average_data(data, time_step, orig_time_step, parameters['inputs']['continuous'])\n",
    "n_days, samples_per_day = compute_stats(data, time_step)\n",
    "past_samples = samples_per_day * parameters['history_size'] // 24\n",
    "t_past = np.arange(past_samples) * time_step / 60\n",
    "t_past -= t_past[-1]\n",
    "t_future = np.arange(samples_per_day) * time_step / 60\n",
    "\n",
    "print(f'Time step: {time_step} minutes.')\n",
    "print(f'Number of days: {n_days}.')\n",
    "print(f'Samples per day: {samples_per_day}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = parameters['inputs']['continuous']\n",
    "if parameters['average_continuous_inputs']:\n",
    "    cols = [col + '_averaged' for col in cols]\n",
    "encoder_file = 'one_hot_encoder.pkl'\n",
    "if 'data2' in input_file:\n",
    "    X,encoder = make_dataset(data, cols, parameters['inputs']['categorical'],\n",
    "                             training_set_max, training_set_min, n_days,\n",
    "                             samples_per_day, full_output=True)\n",
    "    print(f'Saving one-hot encoder to {encoder_file}.')\n",
    "    pickle.dump(encoder, open(encoder_file, 'wb'))\n",
    "else:\n",
    "    if not os.path.isfile(encoder_file):\n",
    "        raise Exception('You must first run this notebook using \"../data/data2.pkl\" as input file.')\n",
    "    encoder = pickle.load(open(encoder_file,'rb'))\n",
    "    X = make_dataset(data, cols, parameters['inputs']['categorical'],\n",
    "                     training_set_max, training_set_min, n_days,\n",
    "                     samples_per_day, encoder)    \n",
    "print(f'The input matrix has {X.shape[0]} rows and {X.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = lambda y,M,m: (m + (M - m) / 2 * (y + 1)) * 1e-3\n",
    "past_measured_scaled = tf.constant(X[-past_samples:, :][np.newaxis, :, :], dtype=tf.float32)\n",
    "past_measured = fun(past_measured_scaled[0,:,0], training_set_max[0], training_set_min[0])\n",
    "future_predicted_scaled = np.ndarray.flatten(\n",
    "    np.concatenate([model.predict(past_measured_scaled) for model in models]))\n",
    "future_predicted = fun(future_predicted_scaled, training_set_max[0], training_set_min[0])\n",
    "\n",
    "prediction_data = {'t_past': t_past, 't_future': t_future,\n",
    "                   'past_measured': past_measured, 'future_predicted': future_predicted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = data['datetime'].iloc[-1]\n",
    "year = dt.year\n",
    "month = dt.month\n",
    "day = dt.day + 1\n",
    "dates_columns = 1\n",
    "dates = [f'{day:02d}/{month:02d}/{year}' for _ in range(samples_per_day - 1)]\n",
    "dates.append(f'{day+1:02d}/{month:02d}/{year}')\n",
    "hours = []\n",
    "for suffix in 'AM','PM':\n",
    "    for hour in [12] + list(range(1, 12)):\n",
    "        for i in range(60 // time_step):\n",
    "            hours.append(f'{hour}:{i*time_step:02d}:00 {suffix}')\n",
    "hours = hours[1:] + hours[:1]\n",
    "\n",
    "filename = f'{year}{month:02d}{day:02d}-{future:.02f}_hours-consumption'\n",
    "if not consumption_only:\n",
    "    filename += '-generation'\n",
    "if with_building_temperature:\n",
    "    filename += '-temperature'\n",
    "\n",
    "with pd.ExcelWriter(filename + '.xlsx') as writer:\n",
    "    if dates_columns == 2:\n",
    "        df = pd.DataFrame({\n",
    "            'date': dates,\n",
    "            'time': hours,\n",
    "            'consumption (w)': future_predicted * 1e3\n",
    "        })\n",
    "    else:\n",
    "        df = pd.DataFrame({\n",
    "            'time': [d + ' ' + h for d,h in zip(dates,hours)],\n",
    "            'consumption (w)': future_predicted * 1e3\n",
    "        })\n",
    "    df.to_excel(writer, sheet_name='building_energy', index=False)\n",
    "df.head()\n",
    "\n",
    "pickle.dump(prediction_data, open(filename + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 1)\n",
    "idx = input_files.index(input_file)\n",
    "if idx > 0:\n",
    "    prev_pred_fname = f'{year}{month:02d}{day-1:02d}-' + '-'.join(filename.split('-')[1:]) + '.pkl'\n",
    "    prev_pred = pickle.load(open(prev_pred_fname, 'rb'))\n",
    "    ax.plot(t_past, prev_pred['future_predicted'], 'm', lw=1, label='Predicted past')\n",
    "    mape = tf.keras.losses.MAPE(past_measured, prev_pred['future_predicted']).numpy()\n",
    "    print(f'MAPE on prediction: {mape:.2f}%.')\n",
    "ax.plot(t_past, past_measured, 'k', lw=1, label='Past')\n",
    "ax.plot(t_future, future_predicted, 'r', lw=1, label='Future')\n",
    "ax.legend(loc='best')\n",
    "dt = 4\n",
    "ax.set_xticks(np.arange(t_past[0] - time_step / 60, t_future[-1] + dt/2, dt))\n",
    "ax.set_xlabel('Time [hours]')\n",
    "ax.set_ylabel('Consumption [MW]')\n",
    "for side in 'right','top':\n",
    "    ax.spines[side].set_visible(False)\n",
    "ax.grid(which='major', axis='y', color=[.6,.6,.6], lw=0.5, linestyle=':')\n",
    "fig.tight_layout()\n",
    "plt.savefig(filename + '.pdf')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "day = '20200210'\n",
    "pkl_files = sorted(glob.glob(f'{day}*.pkl'))\n",
    "cmap = plt.get_cmap('gist_rainbow', len(pkl_files))\n",
    "fig,ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "abbrev = {'consumption': 'cons.', 'generation': 'gen.', 'temperature': 'temp.'}\n",
    "for i,pkl_file in enumerate(pkl_files):\n",
    "    hours = float(re.findall('\\d\\.\\d', pkl_file)[0])\n",
    "    variables = ' '.join([abbrev[var] for var in os.path.splitext(pkl_file)[0].split('-')[2:]])\n",
    "    prediction = pickle.load(open(pkl_file, 'rb'))\n",
    "    if i == 0:\n",
    "        ax.plot(prediction['t_past'], prediction['past_measured'], 'k', lw=2, label='Past')\n",
    "    ax.plot(prediction['t_future'], prediction['future_predicted'],\n",
    "            color=cmap(i), lw=2, label=f'{variables} - {hours} hrs')\n",
    "ax.legend(loc='best')\n",
    "dt = 4\n",
    "ax.set_xticks(np.arange(t_past[0] - time_step / 60, t_future[-1] + dt/2, dt))\n",
    "ax.set_xlabel('Time [hours]')\n",
    "ax.set_ylabel('Consumption [MW]')\n",
    "for side in 'right','top':\n",
    "    ax.spines[side].set_visible(False)\n",
    "ax.grid(which='major', axis='y', color=[.6,.6,.6], lw=0.5, linestyle=':')\n",
    "fig.tight_layout()\n",
    "plt.savefig(day + '.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
