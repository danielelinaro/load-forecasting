{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import tables\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from comet_ml.api import API, APIExperiment\n",
    "from comet_ml.query import Tag\n",
    "\n",
    "if not '..' in sys.path:\n",
    "    sys.path.append('..')\n",
    "from train_LSTM import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = API(api_key = os.environ['COMET_API_KEY'])\n",
    "workspace = 'danielelinaro'\n",
    "project_name = 'load-forecasting'\n",
    "\n",
    "future = 1    # [hours]\n",
    "history = 24  # [hours]\n",
    "num_layers = 3\n",
    "num_neurons = 15\n",
    "\n",
    "query = Tag('LSTM') & \\\n",
    "        Tag(f'future={future}') & \\\n",
    "        Tag(f'history={history}') & \\\n",
    "        Tag(f'{num_layers}_layers') & \\\n",
    "        Tag(f'{num_neurons}_neurons')\n",
    "\n",
    "experiments = api.query(workspace, project_name, query, archived=False)\n",
    "experiment_IDs = []\n",
    "val_loss = []\n",
    "hours_ahead = []\n",
    "for experiment in experiments:\n",
    "    tags = experiment.get_tags()\n",
    "    ID = experiment.id\n",
    "    experiment_IDs.append(ID)\n",
    "    metrics = experiment.get_metrics()\n",
    "    val_loss.append(np.array([float(m['metricValue']) for m in metrics \n",
    "                              if m['metricName'] == 'val_loss']))\n",
    "    hours_ahead.append(float([tag for tag in tags if 'ahead' in tag][0][6:]))\n",
    "    print(f'{ID[:8]} >> hours ahead: {hours_ahead[-1]:4g} ' + \n",
    "          f'validation loss: {val_loss[-1].min():.4f}')\n",
    "hours_ahead = np.array(hours_ahead)\n",
    "idx = np.argsort(hours_ahead)\n",
    "hours_ahead = hours_ahead[idx]\n",
    "val_loss = [val_loss[i] for i in idx]\n",
    "experiment_IDs = [experiment_IDs[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a20536",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_path = '../experiments/LSTM/'\n",
    "n_experiments = len(experiment_IDs)\n",
    "training_set_min = []\n",
    "training_set_max = []\n",
    "models = []\n",
    "for i,experiment_ID in enumerate(experiment_IDs):\n",
    "    checkpoint_path = experiments_path + experiment_ID + '/checkpoints/'\n",
    "    checkpoint_files = glob.glob(checkpoint_path + '*.h5')\n",
    "    epochs = [int(os.path.split(file)[-1].split('.')[1].split('-')[0])\n",
    "              for file in checkpoint_files]\n",
    "    best_checkpoint = checkpoint_files[epochs.index(np.argmin(val_loss[i]) + 1)]\n",
    "    models.append(keras.models.load_model(best_checkpoint, compile=True))\n",
    "\n",
    "parameters = pickle.load(open(experiments_path + experiment_ID + '/parameters.pkl', 'rb'))\n",
    "# we need min and max of the training set to normalize the data\n",
    "training_set_min = parameters['training_set_min']\n",
    "training_set_max = parameters['training_set_max']\n",
    "data_file = '../' + parameters['data_file']\n",
    "time_step = parameters['time_step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf2585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(data_file, 'rb'))\n",
    "df = data['full']['building_energy']\n",
    "orig_time_step = extract_time_step(df)\n",
    "df = add_minute_and_workday(df)\n",
    "df = average_data(df, time_step, orig_time_step, ['consumption', 'generation'])\n",
    "n_days, samples_per_day = compute_stats(df, time_step)\n",
    "t = np.arange(samples_per_day) * time_step / 60\n",
    "\n",
    "print(f'Time step: {time_step} minutes.')\n",
    "print(f'Number of days: {n_days}.')\n",
    "print(f'Samples per day: {samples_per_day}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f21692",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = parameters['inputs']['continuous']\n",
    "if parameters['average_continuous_inputs']:\n",
    "    cols = [col + '_averaged' for col in cols]\n",
    "X = make_dataset(df, cols, parameters['inputs']['categorical'],\n",
    "                training_set_max, training_set_min, n_days, samples_per_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdfc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 103 * 96\n",
    "# today = tf.constant(X[offset : offset + samples_per_day, :][np.newaxis, :, :])\n",
    "today = X[offset : offset + samples_per_day, :]\n",
    "tomorrow = np.zeros(samples_per_day)\n",
    "n_outputs = models[0].outputs[0].shape[1]\n",
    "for i,model in enumerate(models):\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "    tomorrow[i * n_outputs : (i+1) * n_outputs] = model.predict(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = lambda y,M,m: m + (M - m) / 2 * (y + 1)\n",
    "tomorrow_predicted = fun(tomorrow, training_set_max[0], training_set_min[0])\n",
    "tomorrow_actual = fun(X[offset + samples_per_day : offset + 2 * samples_per_day, 0],\n",
    "                     training_set_max[0], training_set_min[0])\n",
    "plt.plot(t, tomorrow_actual, 'k', lw=1, label='Actual')\n",
    "plt.plot(t, tomorrow_predicted, 'r', lw=1, label='Prediction')\n",
    "plt.legend(loc='upper right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
