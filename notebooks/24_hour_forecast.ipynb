{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import tables\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from comet_ml.api import API, APIExperiment\n",
    "from comet_ml.query import Tag\n",
    "\n",
    "if not '..' in sys.path:\n",
    "    sys.path.append('..')\n",
    "from train_LSTM import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de89b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = API(api_key = os.environ['COMET_API_KEY'])\n",
    "workspace = 'danielelinaro'\n",
    "project_name = 'load-forecasting'\n",
    "\n",
    "future = 1    # [hours]\n",
    "history = 24  # [hours]\n",
    "hours_ahead = np.r_[0 : 24]\n",
    "\n",
    "n_neurons = 20\n",
    "n_layers = 3\n",
    "\n",
    "base_query = Tag('LSTM') & Tag(f'future={future}') & Tag(f'history={history}')\n",
    "if n_layers is not None:\n",
    "    base_query &= Tag(f'{n_layers}_layers')\n",
    "if n_neurons is not None:\n",
    "    base_query &= Tag(f'{n_neurons}_neurons')\n",
    "    \n",
    "experiment_IDs = []\n",
    "val_losses = []\n",
    "for hours in hours_ahead:\n",
    "    sys.stdout.write(f'{hours:2.0f} hours ahead ')\n",
    "    sys.stdout.flush()\n",
    "    query = base_query & Tag(f'ahead={hours:.1f}')\n",
    "    experiments = api.query(workspace, project_name, query, archived=False)\n",
    "    msg = f'({len(experiments)} expts): '\n",
    "    n_char = len(msg)\n",
    "    sys.stdout.write(msg)\n",
    "    min_val_loss = 100\n",
    "    for experiment in experiments:\n",
    "        metrics = experiment.get_metrics()\n",
    "        loss = np.array([float(m['metricValue']) for m in metrics if m['metricName'] == 'val_loss'])\n",
    "        if loss.min() < min_val_loss:\n",
    "            val_loss = loss\n",
    "            min_val_loss = loss.min()\n",
    "            tags = experiment.get_tags()\n",
    "            ID = experiment.id\n",
    "    experiment_IDs.append(ID)\n",
    "    val_losses.append(val_loss)\n",
    "    n_neurons = [int(tag.split('_')[0]) for tag in tags if 'neurons' in tag][0]\n",
    "    n_layers = [int(tag.split('_')[0]) for tag in tags if 'layers' in tag][0]\n",
    "    n_epochs = len(val_loss)\n",
    "    print(f'best expt is {ID[:9]} ({n_layers} layers, {n_neurons} neurons, {n_epochs:3d} epochs), ' + \n",
    "          f'validation loss: {min_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a20536",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_path = '../experiments/LSTM/'\n",
    "n_experiments = len(experiment_IDs)\n",
    "training_set_min = []\n",
    "training_set_max = []\n",
    "models = []\n",
    "for i,experiment_ID in enumerate(experiment_IDs):\n",
    "    checkpoint_path = experiments_path + experiment_ID + '/checkpoints/'\n",
    "    checkpoint_files = glob.glob(checkpoint_path + '*.h5')\n",
    "    epochs = [int(os.path.split(file)[-1].split('.')[1].split('-')[0])\n",
    "              for file in checkpoint_files]\n",
    "    best_checkpoint = checkpoint_files[epochs.index(np.argmin(val_losses[i]) + 1)]\n",
    "    models.append(keras.models.load_model(best_checkpoint, compile=True))\n",
    "\n",
    "parameters = pickle.load(open(experiments_path + experiment_ID + '/parameters.pkl', 'rb'))\n",
    "# we need min and max of the training set to normalize the data\n",
    "training_set_min = parameters['training_set_min']\n",
    "training_set_max = parameters['training_set_max']\n",
    "data_file = '../' + parameters['data_file']\n",
    "time_step = parameters['time_step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf2585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(data_file, 'rb'))\n",
    "df = data['full']['building_energy']\n",
    "orig_time_step = extract_time_step(df)\n",
    "df = add_minute_and_workday(df)\n",
    "df = average_data(df, time_step, orig_time_step, ['consumption', 'generation'])\n",
    "n_days, samples_per_day = compute_stats(df, time_step)\n",
    "t = np.arange(samples_per_day) * time_step / 60\n",
    "\n",
    "print(f'Time step: {time_step} minutes.')\n",
    "print(f'Number of days: {n_days}.')\n",
    "print(f'Samples per day: {samples_per_day}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f21692",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = parameters['inputs']['continuous']\n",
    "if parameters['average_continuous_inputs']:\n",
    "    cols = [col + '_averaged' for col in cols]\n",
    "X = make_dataset(df, cols, parameters['inputs']['categorical'],\n",
    "                training_set_max, training_set_min, n_days, samples_per_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdfc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = lambda y,M,m: (m + (M - m) / 2 * (y + 1)) * 1e-3\n",
    "offsets = range(30, 365, 50)\n",
    "n_offsets = len(offsets)\n",
    "fig,ax = plt.subplots(n_offsets, 1, figsize=(6, n_offsets * 2), sharex=True)\n",
    "max_today = 100\n",
    "for i,offset in enumerate(offsets):\n",
    "    start = offset * samples_per_day\n",
    "    stop = (offset + 1) * samples_per_day\n",
    "    today = tf.constant(X[start : stop, :][np.newaxis, :, :], dtype=tf.float32)\n",
    "    tomorrow = np.ndarray.flatten(np.concatenate([model.predict(today) for model in models]))\n",
    "    \n",
    "    today_actual = fun(today[0,:,0], training_set_max[0], training_set_min[0])\n",
    "    tomorrow_predicted = fun(tomorrow, training_set_max[0], training_set_min[0])\n",
    "    tomorrow_actual = fun(X[start + samples_per_day : stop + samples_per_day, 0],\n",
    "                         training_set_max[0], training_set_min[0])\n",
    "    \n",
    "    if np.max(today_actual) < max_today:\n",
    "        max_today = np.max(today_actual)\n",
    "        j = i\n",
    "    ax[i].plot(t - 24, today_actual, color=[0,.5,1], lw=1, label='Real today')\n",
    "    ax[i].plot(t, tomorrow_actual, color=[.3,.3,.3], lw=1, label='Real tomorrow')\n",
    "    ax[i].plot(t, tomorrow_predicted, color=[1,.5,0], lw=2, label='Predicted tomorrow')\n",
    "ax[j].legend(loc='upper left')\n",
    "for a in ax:\n",
    "    a.set_ylabel('Consumption [kW]')\n",
    "    a.set_ylim([1.5, 8.5])\n",
    "    a.set_yticks(np.r_[2 : 9])\n",
    "    a.grid(True, which='major', axis='y', color=[.8,.8,.8], linestyle='-', linewidth=0.5)\n",
    "    for side in 'top','right':\n",
    "        a.spines[side].set_visible(False)\n",
    "ax[-1].set_xlabel('Time [hours]')\n",
    "ax[-1].set_xticks(np.r_[-24 : 25 : 6])\n",
    "fig.tight_layout()\n",
    "fig.savefig('24_hour_forecast.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
