{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a51af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import tables\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from comet_ml.api import API, APIExperiment\n",
    "from comet_ml.query import Tag\n",
    "\n",
    "if not '..' in sys.path:\n",
    "    sys.path.append('..')\n",
    "from train_LSTM import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87089864",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = API(api_key = os.environ['COMET_API_KEY'])\n",
    "workspace = 'danielelinaro'\n",
    "project_name = 'load-forecasting'\n",
    "\n",
    "future = 1    # [hours]\n",
    "history = 24  # [hours]\n",
    "hours_ahead = np.r_[0 : 24]\n",
    "\n",
    "n_neurons = 20\n",
    "n_layers = 3\n",
    "with_building_temperature = False\n",
    "random_initial_weights = True\n",
    "training_data_files = ['data1']\n",
    "\n",
    "base_query = Tag('LSTM') & Tag('_'.join(training_data_files)) & \\\n",
    "    Tag(f'future={future}') & Tag(f'history={history}')\n",
    "if n_layers is not None:\n",
    "    base_query &= Tag(f'{n_layers}_layers')\n",
    "if n_neurons is not None:\n",
    "    base_query &= Tag(f'{n_neurons}_neurons')\n",
    "if with_building_temperature:\n",
    "    base_query &= Tag('building_temperature')\n",
    "if random_initial_weights:\n",
    "    base_query &= Tag('random_initial_weights')\n",
    "else:\n",
    "    base_query &= Tag('initialized_weights')\n",
    "\n",
    "experiments_path = '../experiments/LSTM/'\n",
    "experiment_IDs = []\n",
    "val_losses = []\n",
    "for hours in hours_ahead:\n",
    "    sys.stdout.write(f'{hours:2.0f} hours ahead ')\n",
    "    sys.stdout.flush()\n",
    "    query = base_query & Tag(f'ahead={hours:.1f}')\n",
    "    experiments = api.query(workspace, project_name, query, archived=False)\n",
    "    if not with_building_temperature:\n",
    "        experiments = [expt for expt in experiments if all([tag != 'building_temperature' \\\n",
    "                                                            for tag in expt.get_tags()])]\n",
    "    msg = f'({len(experiments)} expts): '\n",
    "    n_char = len(msg)\n",
    "    sys.stdout.write(msg)\n",
    "    min_val_loss = 100\n",
    "    for experiment in experiments:\n",
    "        metrics = experiment.get_metrics_summary()\n",
    "        loss = [float(m['valueMin']) for m in metrics if m['name'] == 'val_loss'][0]\n",
    "        if loss < min_val_loss:\n",
    "            ID = experiment.id\n",
    "            history = pickle.load(open(experiments_path + ID + '/history.pkl','rb'))\n",
    "            val_loss = history['val_loss']\n",
    "            min_val_loss = loss\n",
    "            tags = experiment.get_tags()\n",
    "    experiment_IDs.append(ID)\n",
    "    val_losses.append(val_loss)\n",
    "    n_neurons = [int(tag.split('_')[0]) for tag in tags if 'neurons' in tag][0]\n",
    "    n_layers = [int(tag.split('_')[0]) for tag in tags if 'layers' in tag][0]\n",
    "    n_epochs = len(val_loss)\n",
    "    print(f'best expt is {ID[:9]} ({n_layers} layers, {n_neurons} neurons, {n_epochs:3d} epochs), ' + \n",
    "          f'validation loss: {min_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a600738",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_experiments = len(experiment_IDs)\n",
    "training_set_min = []\n",
    "training_set_max = []\n",
    "models = []\n",
    "for i,experiment_ID in enumerate(experiment_IDs):\n",
    "    checkpoint_path = experiments_path + experiment_ID + '/checkpoints/'\n",
    "    checkpoint_files = glob.glob(checkpoint_path + '*.h5')\n",
    "    epochs = [int(os.path.split(file)[-1].split('.')[1].split('-')[0])\n",
    "              for file in checkpoint_files]\n",
    "    best_checkpoint = checkpoint_files[epochs.index(np.argmin(val_losses[i]) + 1)]\n",
    "    models.append(keras.models.load_model(best_checkpoint, compile=True))\n",
    "\n",
    "parameters = pickle.load(open(experiments_path + experiment_ID + '/parameters.pkl', 'rb'))\n",
    "# we need min and max of the training set to normalize the data\n",
    "training_set_min = parameters['training_set_min']\n",
    "training_set_max = parameters['training_set_max']\n",
    "data_file = '../' + parameters['data_file']\n",
    "time_step = parameters['time_step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc0ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pickle.load(open(data_file, 'rb'))['full']\n",
    "data = data_full['building_energy'].copy()\n",
    "if 'building_consumption' in parameters['inputs']['continuous']:\n",
    "    data.rename({key: 'building_' + key for key in ('consumption','generation')}, axis='columns', inplace=True)\n",
    "data['building_temperature'] = data_full['building_sensor']['temperature'].copy()\n",
    "orig_time_step = extract_time_step(data)\n",
    "data = add_minute_and_workday(data)\n",
    "data = average_data(data, time_step, orig_time_step, parameters['inputs']['continuous'])\n",
    "n_days, samples_per_day = compute_stats(data, time_step)\n",
    "t = np.arange(samples_per_day) * time_step / 60\n",
    "\n",
    "print(f'Time step: {time_step} minutes.')\n",
    "print(f'Number of days: {n_days}.')\n",
    "print(f'Samples per day: {samples_per_day}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78bde79",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days_training = int(parameters['data_split']['training'] * n_days)\n",
    "n_days_test = int(parameters['data_split']['test'] * n_days)\n",
    "n_days_validation = n_days - n_days_training - n_days_test\n",
    "train_split = n_days_training * samples_per_day\n",
    "validation_split = (n_days_training + n_days_validation) * samples_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = parameters['inputs']['continuous']\n",
    "if parameters['average_continuous_inputs']:\n",
    "    cols = [col + '_averaged' for col in cols]\n",
    "X = make_dataset(data, cols, parameters['inputs']['categorical'],\n",
    "                training_set_max, training_set_min, n_days, samples_per_day)\n",
    "print(f'The input matrix has {X.shape[0]} rows and {X.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30022331",
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = lambda y,M,m: (m + (M - m) / 2 * (y + 1)) * 1e-3\n",
    "\n",
    "mae = np.zeros(n_days_test)\n",
    "mape = np.zeros(n_days_test)\n",
    "for i in range(-1, n_days_test - 1):\n",
    "    start = validation_split + i * samples_per_day\n",
    "    stop = start + samples_per_day\n",
    "    today_measured_scaled = tf.constant(X[start : stop, :][np.newaxis, :, :], dtype=tf.float32)\n",
    "    tomorrow_measured_scaled = X[start + samples_per_day : stop + samples_per_day, 0]\n",
    "    tomorrow_predicted_scaled = np.ndarray.flatten(np.concatenate([model.predict(today_measured_scaled)\n",
    "                                                                   for model in models]))\n",
    "    tomorrow_measured = fun(tomorrow_measured_scaled, training_set_max[0], training_set_min[0])\n",
    "    tomorrow_predicted = fun(tomorrow_predicted_scaled, training_set_max[0], training_set_min[0])\n",
    "    mae[i+1] = tf.keras.losses.MAE(tomorrow_measured, tomorrow_predicted).numpy()\n",
    "    mape[i+1] = tf.keras.losses.MAPE(tomorrow_measured, tomorrow_predicted).numpy()\n",
    "    print(f'Day {i+2:2d}: MAE = {mae[i+1]:6.4f} MAPE = {mape[i+1]:4.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f75356",
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_building_temperature:\n",
    "    output_file = 'errors_with_building_temperature.pkl'\n",
    "else:\n",
    "    output_file = 'errors_without_building_temperature.pkl'\n",
    "pickle.dump({'mae': mae, 'mape': mape}, open(output_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd04f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 2, sharey=True, sharex=True, figsize=(4,3))\n",
    "ax[0].boxplot(mae, notch=True)\n",
    "ax[1].violinplot(mae)\n",
    "for a in ax:\n",
    "    for side in 'top','right':\n",
    "        a.spines[side].set_visible(False)\n",
    "ax[0].set_ylabel('MAE')\n",
    "fig.tight_layout()\n",
    "if with_building_temperature:\n",
    "    fig.savefig('errors_with_building_temperature.pdf')\n",
    "else:\n",
    "    fig.savefig('errors_without_building_temperature.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets = range(n_days_training + n_days_validation, 364, 3)\n",
    "n_offsets = len(offsets)\n",
    "fig,ax = plt.subplots(n_offsets, 1, figsize=(6, n_offsets * 2), sharex=True)\n",
    "max_today = 100\n",
    "for i,offset in enumerate(offsets):\n",
    "    start = offset * samples_per_day\n",
    "    stop = (offset + 1) * samples_per_day\n",
    "    today = tf.constant(X[start : stop, :][np.newaxis, :, :], dtype=tf.float32)\n",
    "    tomorrow = np.ndarray.flatten(np.concatenate([model.predict(today) for model in models]))\n",
    "    \n",
    "    today_actual = fun(today[0,:,0], training_set_max[0], training_set_min[0])\n",
    "    tomorrow_predicted = fun(tomorrow, training_set_max[0], training_set_min[0])\n",
    "    tomorrow_actual = fun(X[start + samples_per_day : stop + samples_per_day, 0],\n",
    "                         training_set_max[0], training_set_min[0])\n",
    "    \n",
    "    if np.max(today_actual) < max_today:\n",
    "        max_today = np.max(today_actual)\n",
    "        j = i\n",
    "    ax[i].plot(t - 24, today_actual, color=[0,.5,1], lw=1, label='Real today')\n",
    "    ax[i].plot(t, tomorrow_actual, color=[.3,.3,.3], lw=1, label='Real tomorrow')\n",
    "    ax[i].plot(t, tomorrow_predicted, color=[1,.5,0], lw=2, label='Predicted tomorrow')\n",
    "ax[j].legend(loc='upper left')\n",
    "for a in ax:\n",
    "    a.set_ylabel('Consumption [kW]')\n",
    "    a.set_ylim([1.5, 8.5])\n",
    "    a.set_yticks(np.r_[2 : 9])\n",
    "    a.grid(True, which='major', axis='y', color=[.8,.8,.8], linestyle='-', linewidth=0.5)\n",
    "    for side in 'top','right':\n",
    "        a.spines[side].set_visible(False)\n",
    "ax[-1].set_xlabel('Time [hours]')\n",
    "ax[-1].set_xticks(np.r_[-24 : 25 : 6])\n",
    "fig.tight_layout()\n",
    "if with_building_temperature:\n",
    "    fig.savefig('24_hour_forecast_with_building_temperature.pdf')\n",
    "else:\n",
    "    fig.savefig('24_hour_forecast_without_building_temperature.pdf')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
